{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import json\n",
    "import tokenize\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source code extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments(source_code):\n",
    "    comments = []\n",
    "    tokens = tokenize.tokenize(BytesIO(source_code.encode(\"utf-8\")).readline)\n",
    "    for toknum, tokval, _, _, _ in tokens:\n",
    "        if toknum == tokenize.COMMENT:\n",
    "            comments.append(tokval.strip(\"# \").strip())\n",
    "    return comments\n",
    "\n",
    "def extract_docstrings_and_defs(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        source = f.read()\n",
    "\n",
    "    tree = ast.parse(source)\n",
    "    results = []\n",
    "    module_docstring = ast.get_docstring(tree)\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "            name = node.name\n",
    "            docstring = ast.get_docstring(node)\n",
    "            node_type = \"function\" if isinstance(node, ast.FunctionDef) else \"class\"\n",
    "            source_lines = source.splitlines()\n",
    "            start_line = node.lineno - 1  # ast 行号从1开始，列表索引从0开始\n",
    "            end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line\n",
    "            source_code = '\\n'.join(source_lines[start_line:end_line])\n",
    "            results.append({\n",
    "                \"type\": node_type,\n",
    "                \"name\": name,\n",
    "                \"docstring\": docstring or \"\",\n",
    "                \"source_code\": source_code,\n",
    "                \"file_docstring\": module_docstring\n",
    "            })\n",
    "\n",
    "    comments = extract_comments(source)\n",
    "    return results, comments\n",
    "\n",
    "def generate_qa_from_entry(entry):\n",
    "    name = entry[\"name\"]\n",
    "    doc = entry[\"docstring\"]\n",
    "    if not doc:\n",
    "        return None\n",
    "\n",
    "    # question = f\"What does the {entry['type']} `{name}` do?\"\n",
    "    # answer = doc.strip()\n",
    "    source_code = entry.get(\"source_code\", \"\")\n",
    "    file_docstring = entry.get(\"file_docstring\", \"\")\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"docstring\": doc.strip(),\n",
    "        \"file_docstring\": file_docstring,\n",
    "        \"source\": \"source_code\",\n",
    "        \"type\": entry[\"type\"],\n",
    "        \"code\": source_code\n",
    "    }\n",
    "\n",
    "def process_directory(dir_path):\n",
    "    qa_pairs = []\n",
    "    for root, _, files in tqdm(os.walk(dir_path)):\n",
    "        for file in tqdm(files):\n",
    "            if file.endswith(\".py\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    entries, comments = extract_docstrings_and_defs(full_path)\n",
    "                    for entry in entries:\n",
    "                        qa = generate_qa_from_entry(entry)\n",
    "                        if qa:\n",
    "                            qa[\"file\"] = full_path\n",
    "                            qa_pairs.append(qa)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to parse {full_path}: {e}\")\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/home/cc/transformers/src/transformers\"\n",
    "qa_data = process_directory(directory)\n",
    "\n",
    "# 保存结果为 JSONL 文件\n",
    "with open(\"source_code_qa.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for qa in qa_data:\n",
    "        f.write(json.dumps(qa, indent=4, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Extracted {len(qa_data)} QA pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git commit extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仓库路径：替换为你本地 transformers 的路径\n",
    "REPO_PATH = \"/home/cc/transformers\"\n",
    "repo = Repo(REPO_PATH)\n",
    "\n",
    "output = []\n",
    "\n",
    "# 遍历最近 N 个 commit（可调整）\n",
    "for commit in repo.iter_commits('main', max_count=100):\n",
    "    commit_data = {\n",
    "        \"commit_hash\": commit.hexsha,\n",
    "        \"author\": commit.author.name,\n",
    "        \"date\": commit.committed_datetime.isoformat(),\n",
    "        \"message\": commit.message.strip()\n",
    "    }\n",
    "\n",
    "    # 获取 diff 的简要变化（可设置为 full_diff=True 看更多上下文）\n",
    "    diffs = commit.diff(commit.parents[0] if commit.parents else None, create_patch=True)\n",
    "\n",
    "    diff_texts = []\n",
    "    for diff in diffs:\n",
    "        try:\n",
    "            diff_texts.append(diff.diff.decode(\"utf-8\", errors=\"ignore\"))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    diff_summary = \"\\n\".join(diff_texts)\n",
    "    commit_data[\"diff_summary\"] = diff_summary\n",
    "\n",
    "    # 构造 QA 对\n",
    "    # qa_item = {\n",
    "    #     \"question\": f\"What changed in commit {commit.hexsha[:7]}?\",\n",
    "    #     \"answer\": f\"{commit.message.strip()}\\n\\nSummary of changes:\\n{diff_summary[:1000]}...\",\n",
    "    #     \"source\": \"git_commit\",\n",
    "    #     \"metadata\": commit_data\n",
    "    # }\n",
    "    qa_item = {\n",
    "        \"question\": f\"What changed in commit {commit.hexsha[:7]}?\",\n",
    "        \"answer\": f\"{commit.message.strip()}\",\n",
    "        \"source\": \"git_commit\",\n",
    "        \"metadata\": commit_data\n",
    "    }\n",
    "\n",
    "    output.append(qa_item)\n",
    "\n",
    "# 保存为 JSONL\n",
    "with open(\"qa_from_commits.jsonl\", \"w\") as f:\n",
    "    for item in output:\n",
    "        f.write(json.dumps(item, indent=4, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github issue extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "g_token = input(\"Enter your github token: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Check fork\n",
      "Body: # What does this PR do?\n",
      "\n",
      "<!--\n",
      "Congratulations! You've made it this far! You're not quite done yet though.\n",
      "\n",
      "Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\n",
      "\n",
      "Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n",
      "\n",
      "Once you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\n",
      "-->\n",
      "\n",
      "<!-- Remove if not applicable -->\n",
      "\n",
      "Fixes # (issue)\n",
      "\n",
      "\n",
      "## Before submitting\n",
      "- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n",
      "- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\n",
      "      Pull Request section?\n",
      "- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\n",
      "      to it if that's the case.\n",
      "- [ ] Did you make sure to update the documentation with your changes? Here are the\n",
      "      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\n",
      "      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\n",
      "- [ ] Did you write any new necessary tests?\n",
      "\n",
      "\n",
      "## Who can review?\n",
      "\n",
      "Anyone in the community is free to review the PR once the tests have passed. Feel free to tag\n",
      "members/contributors who may be interested in your PR.\n",
      "\n",
      "<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\n",
      "\n",
      " If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\n",
      " Please tag fewer than 3 people.\n",
      "\n",
      "Models:\n",
      "\n",
      "- text models: @ArthurZucker\n",
      "- vision models: @amyeroberts, @qubvel\n",
      "- speech models: @eustlb\n",
      "- graph models: @clefourrier\n",
      "\n",
      "Library:\n",
      "\n",
      "- flax: @gante and @Rocketknight1\n",
      "- generate: @zucchini-nlp (visual-language models) or @gante (all others)\n",
      "- pipelines: @Rocketknight1\n",
      "- tensorflow: @gante and @Rocketknight1\n",
      "- tokenizers: @ArthurZucker\n",
      "- trainer: @zach-huggingface and @SunMarc\n",
      "- chat templates: @Rocketknight1\n",
      "\n",
      "Integrations:\n",
      "\n",
      "- deepspeed: HF Trainer/Accelerate: @SunMarc @zach-huggingface\n",
      "- ray/raytune: @richardliaw, @amogkam\n",
      "- Big Model Inference: @SunMarc\n",
      "- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber\n",
      "\n",
      "Documentation: @stevhliu\n",
      "\n",
      "HF projects:\n",
      "\n",
      "- accelerate: [different repo](https://github.com/huggingface/accelerate)\n",
      "- datasets: [different repo](https://github.com/huggingface/datasets)\n",
      "- diffusers: [different repo](https://github.com/huggingface/diffusers)\n",
      "- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\n",
      "\n",
      "Maintained examples (not research project or legacy):\n",
      "\n",
      "- Flax: @Rocketknight1\n",
      "- PyTorch: See Models above and tag the person corresponding to the modality of the example.\n",
      "- TensorFlow: @Rocketknight1\n",
      "\n",
      " -->\n",
      "\n",
      "Title: [do not merge] ci check\n",
      "Body: # What does this PR do?\n",
      "\n",
      "<!--\n",
      "Congratulations! You've made it this far! You're not quite done yet though.\n",
      "\n",
      "Once merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\n",
      "\n",
      "Then, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\n",
      "\n",
      "Once you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\n",
      "-->\n",
      "\n",
      "<!-- Remove if not applicable -->\n",
      "\n",
      "Fixes # (issue)\n",
      "\n",
      "\n",
      "## Before submitting\n",
      "- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\n",
      "- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\n",
      "      Pull Request section?\n",
      "- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\n",
      "      to it if that's the case.\n",
      "- [ ] Did you make sure to update the documentation with your changes? Here are the\n",
      "      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\n",
      "      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\n",
      "- [ ] Did you write any new necessary tests?\n",
      "\n",
      "\n",
      "## Who can review?\n",
      "\n",
      "Anyone in the community is free to review the PR once the tests have passed. Feel free to tag\n",
      "members/contributors who may be interested in your PR.\n",
      "\n",
      "<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\n",
      "\n",
      " If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\n",
      " Please tag fewer than 3 people.\n",
      "\n",
      "Models:\n",
      "\n",
      "- text models: @ArthurZucker\n",
      "- vision models: @amyeroberts, @qubvel\n",
      "- speech models: @eustlb\n",
      "- graph models: @clefourrier\n",
      "\n",
      "Library:\n",
      "\n",
      "- flax: @gante and @Rocketknight1\n",
      "- generate: @zucchini-nlp (visual-language models) or @gante (all others)\n",
      "- pipelines: @Rocketknight1\n",
      "- tensorflow: @gante and @Rocketknight1\n",
      "- tokenizers: @ArthurZucker\n",
      "- trainer: @zach-huggingface and @SunMarc\n",
      "- chat templates: @Rocketknight1\n",
      "\n",
      "Integrations:\n",
      "\n",
      "- deepspeed: HF Trainer/Accelerate: @SunMarc @zach-huggingface\n",
      "- ray/raytune: @richardliaw, @amogkam\n",
      "- Big Model Inference: @SunMarc\n",
      "- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber\n",
      "\n",
      "Documentation: @stevhliu\n",
      "\n",
      "HF projects:\n",
      "\n",
      "- accelerate: [different repo](https://github.com/huggingface/accelerate)\n",
      "- datasets: [different repo](https://github.com/huggingface/datasets)\n",
      "- diffusers: [different repo](https://github.com/huggingface/diffusers)\n",
      "- rust tokenizers: [different repo](https://github.com/huggingface/tokenizers)\n",
      "\n",
      "Maintained examples (not research project or legacy):\n",
      "\n",
      "- Flax: @Rocketknight1\n",
      "- PyTorch: See Models above and tag the person corresponding to the modality of the example.\n",
      "- TensorFlow: @Rocketknight1\n",
      "\n",
      " -->\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = Github(g_token)  # 用你的 GitHub Token\n",
    "\n",
    "repo = g.get_repo(\"huggingface/transformers\")\n",
    "issues = repo.get_issues(state=\"closed\")  # 可加过滤条件\n",
    "# issues[0].__dict__\n",
    "# real_issues = [i for i in all_issues if not i.pull_request]\n",
    "for issue in issues[:2]:\n",
    "    print(\"Title:\", issue.title)\n",
    "    print(\"Body:\", issue.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "qa_pairs = []\n",
    "for issue in issues:\n",
    "    if issue.pull_request:\n",
    "        continue\n",
    "    # print(\"********************Issue********************\")\n",
    "    # print(\"Title:\", issue.title)\n",
    "    # print(\"Body:\", issue.body)\n",
    "    comments = issue.get_comments()\n",
    "    # print(\"********************Comments********************\")\n",
    "    comments_list = []\n",
    "    for comment in comments:\n",
    "        comments_list.append(\n",
    "            {\n",
    "                \"comment_author\": comment.user.login,\n",
    "                \"comment_body\": comment.body\n",
    "            }\n",
    "        )\n",
    "        # print(\"Comment by\", comment.user.login)\n",
    "        # print(comment.body)\n",
    "    qa_pair = {\n",
    "        \"Issue Title\": issue.title.strip(),\n",
    "        \"Issue Body\": issue.body.strip(),\n",
    "        \"Issue Comments\": comments_list,\n",
    "        \"source\": \"github_issue\",\n",
    "        \"metadata\": {\n",
    "            \"issue_number\": issue.number,\n",
    "            \"url\": issue.html_url,\n",
    "            \"created_at\": str(issue.created_at)\n",
    "        }\n",
    "    }\n",
    "    qa_pairs.append(qa_pair)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "\n",
    "\n",
    "with open(\"qa_from_issues.jsonl\", \"w\") as f:\n",
    "    for qa in qa_pairs:\n",
    "        f.write(json.dumps(qa, indent=4, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PR and Code Review Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PRS = 10  # 设定最多提取几个 PR，避免 API rate limit\n",
    "OUTPUT_FILE = \"huggingface_pr_data.jsonl\"\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for pr in repo.get_pulls(state=\"closed\", sort=\"created\", direction=\"desc\"):\n",
    "        if MAX_PRS <= 0:\n",
    "            break\n",
    "        MAX_PRS -= 1\n",
    "\n",
    "        pr_data = {\n",
    "            \"pr_number\": pr.number,\n",
    "            \"title\": pr.title,\n",
    "            \"body\": pr.body,\n",
    "            \"user\": pr.user.login,\n",
    "            \"created_at\": str(pr.created_at),\n",
    "            \"merged\": pr.merged,\n",
    "            \"merge_commit_sha\": pr.merge_commit_sha,\n",
    "            \"files\": [],\n",
    "            \"review_comments\": [],\n",
    "            \"general_comments\": [],\n",
    "        }\n",
    "\n",
    "        # PR 变更文件\n",
    "        try:\n",
    "            for file in pr.get_files():\n",
    "                pr_data[\"files\"].append({\n",
    "                    \"filename\": file.filename,\n",
    "                    \"status\": file.status,\n",
    "                    \"patch\": file.patch if hasattr(file, \"patch\") else None\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch files for PR #{pr.number}: {e}\")\n",
    "\n",
    "        # Code Review 评论\n",
    "        try:\n",
    "            for comment in pr.get_review_comments():\n",
    "                pr_data[\"review_comments\"].append({\n",
    "                    \"user\": comment.user.login,\n",
    "                    \"path\": comment.path,\n",
    "                    \"line\": comment.position,\n",
    "                    \"body\": comment.body\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch comments for PR #{pr.number}: {e}\")\n",
    "        \n",
    "        try:\n",
    "            issue = repo.get_issue(number=pr.number)\n",
    "            for comment in issue.get_comments():\n",
    "                pr_data[\"general_comments\"].append({\n",
    "                    \"user\": comment.user.login,\n",
    "                    \"created_at\": str(comment.created_at),\n",
    "                    \"body\": comment.body\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"[通用评论出错] PR #{pr.number}: {e}\")\n",
    "        # 保存为 JSONL 行\n",
    "        f_out.write(json.dumps(pr_data, indent=4, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:00<00:00, 3856.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers\n",
      "Agents & Tools\n",
      "Agents\n",
      "Agent\n",
      "class transformers.Agent\n",
      "CodeAgent\n",
      "class transformers.CodeAgent\n",
      "React agents\n",
      "class transformers.ReactAgent\n",
      "class transformers.ReactJsonAgent\n",
      "class transformers.ReactCodeAgent\n",
      "ManagedAgent\n",
      "class transformers.ManagedAgent\n",
      "Tools\n",
      "load_tool\n",
      "tool\n",
      "Tool\n",
      "class transformers.Tool\n",
      "Toolbox\n",
      "class transformers.Toolbox\n",
      "PipelineTool\n",
      "class transformers.PipelineTool\n",
      "launch_gradio_demo\n",
      "stream_to_gradio\n",
      "ToolCollection\n",
      "class transformers.ToolCollection\n",
      "Engines\n",
      "TransformersEngine\n",
      "class transformers.TransformersEngine\n",
      "HfApiEngine\n",
      "class transformers.HfApiEngine\n",
      "Agent Types\n",
      "AgentText\n",
      "class transformers.agents.agent_types.AgentText\n",
      "AgentImage\n",
      "class transformers.agents.agent_types.AgentImage\n",
      "AgentAudio\n",
      "class transformers.agents.agent_types.AgentAudio\n",
      "✅ Done! Extracted 37 QA pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "def fetch_hf_doc(url):\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def extract_sections(soup):\n",
    "    qa_pairs = []\n",
    "    headers = soup.find_all(['h1', 'h2', 'h3'])\n",
    "\n",
    "    for header in tqdm(headers):\n",
    "        title = header.get_text().strip()\n",
    "        print(title)\n",
    "        content = []\n",
    "        next_sibling = header.find_next_sibling()\n",
    "\n",
    "        # Collect paragraphs and lists until the next header\n",
    "        while next_sibling and next_sibling.name not in ['h1', 'h2', 'h3']:\n",
    "            # if next_sibling.name in ['p', 'ul', 'ol', 'pre', 'li']:\n",
    "            content.append(next_sibling.get_text().strip())\n",
    "            next_sibling = next_sibling.find_next_sibling()\n",
    "\n",
    "        text = \"\\n\".join(content).strip()\n",
    "        if text:\n",
    "            qa_pairs.append(\n",
    "                {\n",
    "                    \"header\": title,\n",
    "                    \"content\": text,\n",
    "                    \"source\": \"huggingface_doc\",\n",
    "                    \"url\": soup.title.string if soup.title else \"N/A\"\n",
    "                }\n",
    "            )\n",
    "            # question, answer = make_qa_from_section(title, text)\n",
    "\n",
    "            # if question:\n",
    "            #     qa_pairs.append({\n",
    "            #         \"question\": question,\n",
    "            #         \"answer\": answer,\n",
    "            #         \"source\": \"huggingface_doc\",\n",
    "            #         \"metadata\": {\n",
    "            #             \"section_title\": title,\n",
    "            #             \"url\": soup.title.string if soup.title else \"N/A\"\n",
    "            #         }\n",
    "            #     })\n",
    "\n",
    "    return qa_pairs\n",
    "\n",
    "def make_qa_from_section(title, text):\n",
    "    \"\"\"\n",
    "    基于标题和内容生成问题模板\n",
    "    \"\"\"\n",
    "    if len(text.split()) < 5:\n",
    "        return None, None\n",
    "\n",
    "    # 简单模板化问题构造\n",
    "    if \"tokenizer\" in title.lower():\n",
    "        q = f\"What is {title} in HuggingFace Transformers?\"\n",
    "    elif title.lower().startswith(\"how\"):\n",
    "        q = title + \"?\"\n",
    "    elif \"parameters\" in title.lower():\n",
    "        q = f\"What parameters does {title} include?\"\n",
    "    else:\n",
    "        q = f\"What does '{title}' refer to in Transformers?\"\n",
    "\n",
    "    return q, text\n",
    "\n",
    "def save_to_jsonl(data, filename=\"hf_qa_output.jsonl\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, indent=4, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "url = \"https://huggingface.co/docs/transformers/en/main_classes/agent#transformers.Agent\"  # 你可以替换成其他页面\n",
    "soup = fetch_hf_doc(url)\n",
    "qa_pairs = extract_sections(soup)\n",
    "save_to_jsonl(qa_pairs)\n",
    "print(f\"✅ Done! Extracted {len(qa_pairs)} QA pairs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformerQA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
