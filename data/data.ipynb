{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtokenize\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "import json\n",
    "import tokenize\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments(source_code):\n",
    "    comments = []\n",
    "    tokens = tokenize.tokenize(BytesIO(source_code.encode(\"utf-8\")).readline)\n",
    "    for toknum, tokval, _, _, _ in tokens:\n",
    "        if toknum == tokenize.COMMENT:\n",
    "            comments.append(tokval.strip(\"# \").strip())\n",
    "    return comments\n",
    "\n",
    "def extract_docstrings_and_defs(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        source = f.read()\n",
    "\n",
    "    tree = ast.parse(source)\n",
    "    results = []\n",
    "    module_docstring = ast.get_docstring(tree)\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "            name = node.name\n",
    "            docstring = ast.get_docstring(node)\n",
    "            node_type = \"function\" if isinstance(node, ast.FunctionDef) else \"class\"\n",
    "            source_lines = source.splitlines()\n",
    "            start_line = node.lineno - 1  # ast 行号从1开始，列表索引从0开始\n",
    "            end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line\n",
    "            source_code = '\\n'.join(source_lines[start_line:end_line])\n",
    "            results.append({\n",
    "                \"type\": node_type,\n",
    "                \"name\": name,\n",
    "                \"docstring\": docstring or \"\",\n",
    "                \"source_code\": source_code,\n",
    "                \"file_docstring\": module_docstring\n",
    "            })\n",
    "\n",
    "    comments = extract_comments(source)\n",
    "    return results, comments\n",
    "\n",
    "def generate_qa_from_entry(entry):\n",
    "    name = entry[\"name\"]\n",
    "    doc = entry[\"docstring\"]\n",
    "    if not doc:\n",
    "        return None\n",
    "\n",
    "    # question = f\"What does the {entry['type']} `{name}` do?\"\n",
    "    # answer = doc.strip()\n",
    "    source_code = entry.get(\"source_code\", \"\")\n",
    "    file_docstring = entry.get(\"file_docstring\", \"\")\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"docstring\": doc.strip(),\n",
    "        \"file_docstring\": file_docstring,\n",
    "        \"source\": \"source_code\",\n",
    "        \"type\": entry[\"type\"],\n",
    "        \"code\": source_code\n",
    "    }\n",
    "\n",
    "def process_directory(dir_path):\n",
    "    qa_pairs = []\n",
    "    for root, _, files in tqdm(os.walk(dir_path)):\n",
    "        for file in tqdm(files):\n",
    "            if file.endswith(\".py\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    entries, comments = extract_docstrings_and_defs(full_path)\n",
    "                    for entry in entries:\n",
    "                        qa = generate_qa_from_entry(entry)\n",
    "                        if qa:\n",
    "                            qa[\"file\"] = full_path\n",
    "                            qa_pairs.append(qa)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to parse {full_path}: {e}\")\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 29 QA pairs.\n"
     ]
    }
   ],
   "source": [
    "directory = \"/home/cc/transformers/src/transformers\"\n",
    "qa_data = process_directory(directory)\n",
    "\n",
    "# 保存结果为 JSONL 文件\n",
    "with open(\"source_code_qa.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for qa in qa_data:\n",
    "        f.write(json.dumps(qa, indent=4, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Extracted {len(qa_data)} QA pairs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformerQA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
